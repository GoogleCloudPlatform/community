---
title: Send a Slack notification when a Dataflow job fails
description: Learn how to receive notifications in your chat system when a Dataflow job fails.
author: gmogr,MrTrustor
tags: chatops
date_published: 2021-03-25
---

Grigory Movsesyan | Cloud Engineer | Google

Théo Chamley | Cloud Solutions Architect | Google

<p style="background-color:#CAFACA;"><i>Contributed by Google employees.</i></p>

Receiving notifications when something fails is an important part of maintaining computer systems. Sending notifications to a chat system is a common way to
achieve this. Dataflow is a serverless stream and batch data processing service. In this tutorial, you learn how to send notifications to a Slack channel when a
Dataflow job fails.

You use Cloud Logging to send Dataflow error messages to a Cloud Function with Pub/Sub. The Cloud Function then calls a Slack webhook to send the notification.

![Architecture](https://storage.googleapis.com/gcp-community/tutorials/dataflow-notification-slack/architecture.png)

## Objectives

*   Run a successful Dataflow job and examine its logs.
*   Run a failed Dataflow job and examine its logs.
*   Create a log sink.
*   Create a Slack webhook.
*   Build a pipeline to synchronize log messages generated by the Dataflow pipeline to a Pub/Sub topic and create a Cloud Function listening to this topic and
    pushing success or failure messages to a Slack channel.

## Costs

This tutorial uses the following billable components of Google Cloud:

*   Cloud Dataflow
*   Cloud Logging
*   Pub/Sub
*   Cloud Functions
*   Cloud Storage

To generate a cost estimate based on your projected usage, use the [pricing calculator](https://cloud.google.com/products/calculator).

When you finish this tutorial, you can avoid continued billing by deleting the resources that you created.

## Before you begin

1.  [Select or create a Google Cloud project.](https://console.cloud.google.com/cloud-resource-manager)
1.  [Enable billing for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)
1.  Open [Cloud Shell](https://console.cloud.google.com/?cloudshell=true).
1.  In Cloud Shell, set the working project:

        gcloud config set project [PROJECT_ID]
        
    Replace `[PROJECT_ID]` with your project ID.

1.  Enable the APIs needed for this tutorial:

        gcloud services enable dataflow.googleapis.com \
          storage.googleapis.com \
          pubsub.googleapis.com \
          cloudfunctions.googleapis.com \
          logging.googleapis.com \
          cloudbuild.googleapis.com

## Generate Dataflow logs

In this section, you create a Dataflow job that generates logs. This tutorial uses the
[WordCount example](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates#running-the-wordcount-template).

1.  Create a Cloud Storage bucket:

        gsutil mb gs://$(gcloud config get-value project)-wordcount

1.  Run the Dataflow job:

        gcloud dataflow jobs run wordcount \
          --gcs-location gs://dataflow-templates/latest/Word_Count \
          --parameters \
          inputFile=gs://dataflow-samples/shakespeare/kinglear.txt,output=gs://$(gcloud config get-value project)-wordcount/output/my_output

1.  Go to the [**Dataflow** page](https://console.cloud.google.com/dataflow/jobs) in the Cloud Console and click **wordcount** to see the progress of the job.
1.  Wait for the job to finish. This should take no more than 5 minutes.
1.  Go to the [**Logs Explorer**](https://console.cloud.google.com/logs/query).
1.  Enter the following query in the **Query builder**, replacing `[PROJECT_ID]` with your project ID:

        resource.type="dataflow_step"
        logName=("projects/[PROJECT_ID]/logs/dataflow.googleapis.com%2Fjob-message")`
        severity=DEBUG
        resource.labels.job_name=wordcount
        textPayload=~"^Executing success step success.*$"

1.  Click **Run query**
1.  Verify the presence of the `Executing success step success47` message.

    This message indicates that the job has successfully finished.
    
1.  In Cloud Shell, run a new Dataflow job that will fail:

        gcloud dataflow jobs run wordcount-fail \
          --gcs-location gs://dataflow-templates/latest/Word_Count \
          --parameters \
          inputFile=gs://non-existing-path,output=gs://$(gcloud config get-value project)-wordcount/output/my_output
 
     Because the `inputFile` value for this job refers to a non-existent file, the job will fail.

1.  Go to the [**Dataflow** page](https://console.cloud.google.com/dataflow/jobs) in the Cloud Console and click **wordcount** to see the progress of the job.
1.  Wait for the job to fail. This should take no more than 5 minutes.
1.  Go to the [**Logs Explorer**](https://console.cloud.google.com/logs/query).
1.  Enter the following query in the **Query builder**, replacing `[PROJECT_ID]` with your project ID:

        resource.type="dataflow_step"
        logName=("projects/[PROJECT_ID]/logs/dataflow.googleapis.com%2Fjob-message")
        severity=ERROR
        resource.labels.job_name=wordcount-fail

1.  Click **Run query**.

1.  Check that you see the `Workflow failed. Causes: [...]` error message.

## Create a log sink

In this section, you create a log sink to send relevant logs to a Cloud Function.

1.  Go to the [**Logs Router** page](https://console.cloud.google.com/logs/router).
1.  Click **Create Sink**.
1.  Enter `dataflow-failure` in the **Sink name** field, and click **Next**.
1.  In the **Sink destination** section, do the following:
    1.  Select **Cloud Pub/sub topic** as the sink service.
    1.  Select **Create new Cloud Pub/Sub topic** in the **Select Cloud Pub/Sub topic** field.
    1.  Enter `dataflow-failure` in the **Name** field.
    1.  Click **Create**.
    1.  Click **Next**.
1.  In the **Build inclusion filter** field, enter the following query, replacing `[PROJECT_ID]` with your project ID:

        resource.type="dataflow_step" AND logName=("projects/[PROJECT_ID]/logs/dataflow.googleapis.com%2Fjob-message") AND ((severity=ERROR AND textPayload=~"^Workflow failed.*$") OR (severity=DEBUG AND textPayload=~"^Executing success step success.*$"))

1. Click **Create Sink**.

## Create a webhook for Slack

In this section, you create the Slack webhook used to send notifications.

**Note:** Alternatively, you can create a Google Chat webhook. The webhook APIs for Slack and Google Chat are similar, and the rest of the tutorial should
work without any modification.

1.  Create a webhook for Slack following the [Slack documentation](https://api.slack.com/messaging/webhooks).
1.  In Cloud Shell, test your webhook, replacing `[WEBOOK_URL]` with your webhook’s URL:

        curl -d '{"text":"Hello, world"}' -H "Content-Type:application/json; charset=UTF-8" "[WEBHOOK_URL]"

1. Check the configured Slack channel for the new “Hello, world” message.

## Create the Cloud Function

In this section, you create a Cloud Function that reads the Dataflow error messages from Cloud Pub/Sub and calls the Slack webhook.

1.  Go to the [**Cloud Functions** page](https://console.cloud.google.com/functions/list).
1.  Click **Create Function**.
1.  Enter `slack-webhook` in the **Function name** field.
1.  Select **Cloud Pub/sub** for **Trigger type**.
1.  Choose the topic that you created in the previous section (which should end with `dataflow-failure`) in the **Select a Cloud Pub/Sub topic** field.
1.  Click **Save**.
1.  Click **Variables, Networking, and Advanced Settings**.
1.  Select the **Environment Variables** tab.
1.  Click **Add Variable** in the **Runtime environment variables** section (_not_ in the **Build environment variables** section).
1.  Enter `SLACK_WEBHOOK` in the **Name** field.
1.  Enter your Slack webhook URL in the **Value** field.
1.  Click **Next**.
1.  Select **Node.js 12** for **Runtime**.
1.  Copy the following code into the `index.js` file using the inline editor.

        exports.helloPubSub = (message, context) => {
          const https = require('https');
          const url = require('url');
          const msg = Buffer.from(message.data, 'base64').toString()
          const msgObj = JSON.parse(msg)
          const data = JSON.stringify({text: msgObj.resource.labels.job_name + ": " + msgObj.textPayload})
          const uri = url.parse(process.env.SLACK_WEBHOOK);
          var options = {
            host: uri.hostname,
            path: `${uri.pathname}${uri.search}`,
            method: "POST",
            headers: {
              'Content-Type': 'application/json',
              'Content-Length': data.length
            }
          };

          const req = https.request(options).on("error", (err) => {
            console.error("Error: ", err.message);
          });
          req.write(data);
          req.end();

        };

1. Click **Deploy**.

## Test the notification pipeline

In this section, you test the whole system from end to end.

1.  In Cloud Shell, run a Dataflow job that will fail:

        gcloud dataflow jobs run wordcount-fail \
          --gcs-location gs://dataflow-templates/latest/Word_Count \
          --parameters \
          inputFile=gs://non-existing-path,output=gs://$(gcloud config get-value project)-wordcount/output/my_output

1. Wait for the error message to appear in the Slack channel. This should take no more than 5 minutes.

## Troubleshooting

If you need to troubleshoot the pipeline, you can simulate a Dataflow log message using the API explorer.

1.  Go to the [API explorer for the Cloud Logging API](https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write).
1.  Enter the following JSON in the **Request body** field, replacing  `[PROJECT_ID]` with your project ID:

        {
          "entries": [
            {
              "logName": "projects/[PROJECT_ID]/logs/dataflow.googleapis.com%2Fjob-message",
              "textPayload": "Executing success step success",
              "severity": "DEBUG",
              "resource": {
                "type": "dataflow_step",
                "labels": {
                  "job_name": "SUCCESSFUL_JOB_NAME"
                }
              }
            }
          ]
        }

1.  Wait for the message to appear in the Slack channel. This should take no more than a few seconds. 

## Cleaning up

You can clean up all of the resources created in this tutorial by
[shutting down the project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects). However, if you used an existing
project that contains resources that you want to keep, you can remove the resources created in this tutorial instead.

1.  In Cloud Shell, delete the log sink:

        gcloud logging sinks delete dataflow-failure -q

1.  Delete the Cloud Pub/Sub topic:

        gcloud pubsub topics delete projects/$(gcloud config get-value project)/topics/dataflow-failure -q

1.  Delete the Cloud Function:

        gcloud functions delete slack-webhook -q

1.  Delete the Cloud Storage bucket:

        gsutil rm -r gs://$(gcloud config get-value project)-wordcount

## What’s next

- Learn how to [troubleshoot and debug your Dataflow pipeline](https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline).
- Learn how to [update an existing pipeline](https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline).
- Learn more about [Google Chat webhooks](https://developers.google.com/hangouts/chat/how-tos/webhooks).
- Try out other Google Cloud features for yourself. Have a look at our [tutorials](https://cloud.google.com/docs/tutorials).
