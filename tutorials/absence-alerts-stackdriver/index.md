---
title: Scaling times series absence alerts with Stackdriver
description: This tutorial describes creating alerts for missing monitoring data with Stackdriver alerts, avoiding duplicate alerts so that you are not overwhelmed with notifications.
author: alexamies
tags: Stackdriver, monitoring
date_published: 2019-10-29
---

This tutorial describes creating alerts for missing monitoring data with Stackdriver alerts, avoiding duplicate alerts so that you are not overwhelmed with notifications. For example, suppose that you have 100 time series and you want to find out when any one of them is missing. If one or two time series are missing, you want exactly one alert. When there is a total outage you still want to get one alert, not 100 alerts. 

The schematic diagram below shows the general flow: The test app divides task processing into multiple partitions, each of which generate a time series, which are sent to Stackdriver. When there is a missing time series, Stackdriver sends an alert to a user.

![schematic diagram](https://storage.googleapis.com/gcp-community/tutorials/absence-alerts-stackdriver/schematic.png)

The instructions are provided for a Linux development environment, such as the
[Google Cloud Shell](https://cloud.google.com/shell/).
However, you can equally run the application on Google Compute Engine, Kubernetes, a serverless environment, or locations outside Google Cloud Platform.

The tutorial assumes that you are familiar with Google Cloud Platform, including Stackdriver Monitoring and Alerting. It builds on the discussion in 
[Alerting policies in depth](https://cloud.google.com/monitoring/alerts/concepts-indepth).

## Objectives

* Learn how to create alerts using the gcloud command line
* Learn to how to minimize the number of alerts generated by reducing multiple time series, detecting when one is missing

## Costs

This tutorial uses billable components of Google Cloud Platform, including:

* Stackdriver Monitoring

Use the 
[Pricing Calculator](https://cloud.google.com/products/calculator)
to generate a cost estimate based on your projected usage. This tutorial only generates a small amount of Stackdriver Monitoring data, which may fall within the free allotment.

## Before you begin

For this tutorial, you need a GCP
[project](https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy#projects).
You can create a new one, or select a project you already created, provided that it is not an existing Stackdriver workspace:

1. Select or create a GCP project.
[GO TO THE MANAGE RESOURCES PAGE](https://console.cloud.google.com/cloud-resource-manager)

2. Enable billing for your project.
[ENABLE BILLING](https://support.google.com/cloud/answer/6293499#enable-billing)

3. In the GCP Console, go to 
[Monitoring](https://console.cloud.google.com/monitoring).
If you have not already created a workspace for this project before, click **New workspace**, and then click **Add**. It takes a few minutes to create the workspace. Click **Alerting | Policies overview**. The list should be empty at this point unless you have created policies previously.

4. Open the [Cloud Shell](https://cloud.google.com/shell/#)
in the Google Cloud Console

5. Clone the code for the project with the command

```shell
git clone https://github.com/GoogleCloudPlatform/professional-services.git
cd professional-services/examples/alert-absence-dedup
```

6. Enable the Stackdriver Monitoring API with the command

```shell
gcloud services enable monitoring.googleapis.com
```

Note that if the project is a "monitored project" in a Stackdriver workspace corresponding to another project, then any alerting policy definitions will need to be written against the associated workspace project rather than the project in which these virtual machines are being created. The instructions in this tutorial assume that the Stackdriver workspace corresponds to the selected project.

When you finish this tutorial, you can avoid continued billing by deleting the resources you created. See
[Cleaning up](#cleaning-up) for more detail.

## Description of the policy

The alert will be created with the
[gcloud alpha monitoring policies](https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/)
command, which uses the Stackdriver Monitoring API.  The specific REST resource used in that API is
[AlertPolicy](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#AlertPolicy).
The policy has two conditions:

**Condition #1: detect when the logic for writing time-series is broken**:

Use a metric absence with a “crossSeriesReducer” that combines all of the time-series from different data sources together. That is: only fire when ALL of the time-series are absent, which would be expected to happen if there is something fundamentally broken with the time-series writing logic with the underlying assumption that all of the data sources share the same logic for reporting the data.

**Condition #2: detect when one or more partitions fail to report data independent of the others**, i.e. a cause other than the time-series writing logic, such as loss of connectivity of a particular instance.

For this condition, we use a
[metric threshold](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#MetricThreshold)
rather than a
[metric absence](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#MetricAbsence)
with a REDUCE_COUNT crossSeriesReducer
[aggregation](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#Aggregation)
that reports a total count of the number of time-series that were found. We apply a threshold to the total count, since we know how many shards there ought to be. Alternatively, we could apply a threshold to an ALIGN_DELTA (or ALIGN_PERCENT_CHANGE) of the REDUCE_COUNT output to detect if the change in the total number of data sources goes down by a certain amount. The former (directly thresholding on the REDUCE_COUNT output) makes sense if you know that a precise number of data sources is expected. The latter (computing an absolute or relative change in number and thresholding on the change) is useful if there isn't a fixed number of data sources, but the overall number of data sources is expected to monotonically increase or is expected to only change slowly, with a non-zero decrease or a very high percent change in number reflecting an issue with the service.

## Detailed steps

Detailed steps for some specific metrics and testing are described here. The gcloud command is used because some of these options are not available in the user interface and for repeatability.

### Deploy the app

The example app is based on the Go code in Custom metrics with OpenCensus. It generates time series for a metric called task_latency_distribution. The app code has been extended to tag the time series with partition labels and run indefinitely.

1. [Download](https://golang.org/dl/)
   and install the latest version of Go.
2. Build the test app with the command

```shell
go build
```

Next we setup authentication for the Stackdriver client library. Note that if you run the test app on a Google Cloud Compute Engine instance, Google Kubernetes Engine, or Google Cloud serverless environment you will not have to create a service account or download the credentials. See
[Setting up authentication](https://cloud.google.com/monitoring/docs/reference/libraries#setting_up_authentication)
for more details.

1. Set the project id in a shell variable

```shell
export GOOGLE_CLOUD_PROJECT=[your project]
```

2. Create a service account:

```shell
SA_NAME=stackdriver-metrics-writer
gcloud iam service-accounts create $SA_NAME \
  --display-name="Stackdriver Metrics Writer" 
```

3. Bind the service account to a policy:

```shell
SA_ID="$SA_NAME@$GOOGLE_CLOUD_PROJECT.iam.gserviceaccount.com"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT \
  --member "serviceAccount:$SA_ID" --role "roles/monitoring.metricWriter"
```

4. Generate a credentials file with an exported variable `GOOGLE_APPLICATION_CREDENTIALS` referring to it:

```shell
mkdir -p ~/.auth
chmod go-rwx ~/.auth
export GOOGLE_APPLICATION_CREDENTIALS=~/.auth/stackdriver_demo_credentials.json 
gcloud iam service-accounts keys create $GOOGLE_APPLICATION_CREDENTIALS \
 --iam-account $SA_ID
```

5. Run the program with three partitions, labelled "1", "2", and "3":

```shell
./alert-absence-demo --labels "1,2,3"
```

This will write three time series with the given labels. A few minutes after starting the app you should be able to see the time series data in the Stackdriver Metric explorer, as in the screenshot below.

![schematic diagram](https://storage.googleapis.com/gcp-community/tutorials/absence-alerts-stackdriver/metrics_explorer.png)

Notice that the time series can be grouped by partition.

### Create a notification channel

The notification channel can be created with the
[gcloud alpha monitoring channels](https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/channels/)
command. Create a notification channel with the command, replacing your email for the shell variable `EMAIL`:

```shell
EMAIL="your email"
CHANNEL=$(gcloud alpha monitoring channels create \
  --channel-labels=email_address=$EMAIL \
  --display-name="Email to project owner" \
  --type=email \
  --format='value(name)')
```

The shell variable `CHANNEL` now contains  the name of the notification channel created, which can be used in creating an alerting policy below.

### Create the alerting policy

This example will use the custom metric `task_latency_distribution` discussed in
[Custom metrics with OpenCensus](https://cloud.google.com/monitoring/custom-metrics/open-census).
The code sample there has been extended to add tags based on a partition label. Each partition generates its own time series.

The alerting policy can be created with the
[gcloud alpha monitoring policies create](https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create)
command. The details of the policy are defined in the file
[alert_policy.json](https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/alert-absence-dedup/alert_policy.json).

In a new shell, type the command

```shell
gcloud alpha monitoring policies create \
  --notification-channels=$CHANNEL \
  --documentation-from-file=policy_doc.md \
  --policy-from-file=alert_policy.json
```

The policy includes the notification channel created above and the
[alert documentation](https://cloud.google.com/monitoring/alerts/using-alerting-ui#documentation)
content in markdown form in file policy_doc.md. This is a  good place to add playbook-like instructions to assist the oncall who responds to the alert. The documentation includes
[template variables](https://cloud.google.com/monitoring/alerts/doc-variables)
to make the content as relevant as possible.

At this point no alerts should be firing. You can check that in the Stackdriver Monitoring console alert policy detail.

### Test the policy

Kill the processes with control-c and restart it with only two partitions:

```shell
./alert-absence-demo --labels "1,2"
```

An alert should be generated in about 5 minutes with a subject like ‘One of the time series is absent,’ like in the screenshot below.

![Absence alert with one time series missing](https://storage.googleapis.com/gcp-community/tutorials/absence-alerts-stackdriver/alert_one_missing.png)

Restart the process with three partitions:

```shell
./alert-absence-demo "1,2,3"
```

After a few minutes the alert should resolve itself.

Stop the process and restart it with only one partition:

```shell
./alert-absence-demo --labels "1"
```

Check that only one alert is fired. An example is shown below.

![Absence alert with two time series missing](https://storage.googleapis.com/gcp-community/tutorials/absence-alerts-stackdriver/alert_two_missing.png)

Start the instances and wait for the incident to be resolved.

Stop the process and do not restart it. You should see an alert that indicates all time series are absent, as in the screenshot below.

![Absence alert with all time series missing](https://storage.googleapis.com/gcp-community/tutorials/absence-alerts-stackdriver/alert_all_missing.png)

## Cleaning up

To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial:

### Delete the project

The easiest way to eliminate billing is to delete the project you created for the tutorial.

To delete the project:

1. In the Cloud Platform Console, go to the Projects page.
[GO TO THE PROJECTS PAGE](https://console.cloud.google.com/iam-admin/projects)

2. In the project list, select the project you want to delete and click **Delete**.

3. In the dialog, type the project ID, and then click **Shut down** to delete the project.

## What's next

* Take a look at some more [Sample alert policies](https://cloud.google.com/monitoring/alerts/policies-in-json)
* Try out other Google Cloud Platform features for yourself. Have a look at our
[tutorials](https://cloud-dot-google-developers.appspot.com/community/tutorials/).
